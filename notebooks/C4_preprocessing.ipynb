{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e29537c-e2f5-4ec3-9d3b-819133f821d4",
   "metadata": {},
   "source": [
    "## Information Retrieval 24/25, University of Pisa\n",
    "### Franco Maria Nardini, Rossano Venturini (francomaria.nardini@isti.cnr.it, rossano.venturini@unipi.it)\n",
    "\n",
    "# Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2a916b-1a2b-49fe-b9aa-38b475128c2b",
   "metadata": {},
   "source": [
    "## C4 Dataset\n",
    "\n",
    "[C4 Dataset](https://huggingface.co/datasets/allenai/c4)\n",
    "\n",
    "A colossal, cleaned version of Common Crawl's web crawl corpus (from Google). Based on Common Crawl dataset: \"https://commoncrawl.org\".\n",
    "\n",
    "We use the processed version of Google's C4 dataset by Allen Institute for AI.\n",
    "They prepared five variants of the data: `en`, `en.noclean`, `en.noblocklist`, `realnewslike`, and `multilingual (mC4)`.\n",
    "\n",
    "For reference, these are the sizes of the variants:\n",
    "\n",
    "- `en`: 305GB\n",
    "- `en.noclean`: 2.3TB\n",
    "- `en.noblocklist`: 380GB\n",
    "- `realnewslike`: 15GB\n",
    "- `multilingual (mC4)`: 9.7TB (108 subsets, one per language)\n",
    "\n",
    "The `en.noblocklist` variant is exactly the same as the en variant, except we turned off the so-called \"badwords filter\", which removes all documents that contain words from the lists at https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c91ad1d4-e5b6-422c-aade-1f234815801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/rossano/python_env/lib/python3.11/site-packages (2.20.0)\n",
      "Requirement already satisfied: nltk in /home/rossano/python_env/lib/python3.11/site-packages (3.9.1)\n",
      "Collecting unidecode\n",
      "  Obtaining dependency information for unidecode from https://files.pythonhosted.org/packages/84/b7/6ec57841fb67c98f52fc8e4a2d96df60059637cba077edc569a302a8ffc7/Unidecode-1.3.8-py3-none-any.whl.metadata\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (3.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (0.24.0)\n",
      "Requirement already satisfied: packaging in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/rossano/python_env/lib/python3.11/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: click in /home/rossano/python_env/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/rossano/python_env/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/rossano/python_env/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/rossano/python_env/lib/python3.11/site-packages (from aiohttp->datasets) (2.3.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/rossano/python_env/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/rossano/python_env/lib/python3.11/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/rossano/python_env/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/rossano/python_env/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/rossano/python_env/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/rossano/python_env/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/rossano/python_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rossano/python_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/rossano/python_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/rossano/python_env/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/rossano/python_env/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/rossano/python_env/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/rossano/python_env/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/rossano/python_env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.8\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets nltk unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234d4f2-962f-44cc-837b-72a16d2758cf",
   "metadata": {},
   "source": [
    "#### We download 4 out of 1024 files of size ~318Mb compressed each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12584a49-b471-4e6f-b5f6-f82bc7562b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "c4_subset = load_dataset(\"allenai/c4\", data_files=\"en/c4-train.0102*-of-01024.json.gz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf54a4d1-b9b3-4ff5-b629-f7ebfee4d158",
   "metadata": {},
   "source": [
    "#### Get a list of URls and a list of corresponding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a91e23-8379-4762-8762-4ad1c6eb1241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1425269\n",
      "Number of characters: 3065881920\n"
     ]
    }
   ],
   "source": [
    "urls = [x['url'] for x in c4_subset[\"train\"]]\n",
    "documents = [x['text'] for doc_id, x in enumerate(c4_subset[\"train\"])]\n",
    "\n",
    "print(f\"Number of documents: {len(urls)}\")\n",
    "print(f\"Number of characters: {sum(len(x) for x in documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8934d9ae-91d4-4f83-aeba-4d54c0667789",
   "metadata": {},
   "outputs": [],
   "source": [
    "c4_subset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e598d62-9bf0-4026-ba81-d31b168a793d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://americanhealthandbeauty.com/articles/2704/non-surgical-fat-reduction--zerona-vs-zeltiq\n",
      "Liposuction has remained one of the most popular cosmetic surgeries for years as people turn to their doctors to remove the fat that diet and exercise can't seem to touch. Recently, there has been a trend towards less invasive aesthetic options as lasers and fillers replace facelifts and laser lipo takes center stage with traditional liposuction. There are two devices, both currently undergoing FDA testing, which could replace fat reduction surgery altogether. They are Zerona and Zeltiq, and they are pain free treatments to cut the fat.\n",
      "Made by Erchonia, the Zerona laser is not exactly a new concept. Newport Beach Zerona physician Dr. Thomas Barnes has been using this low level laser therapy device in his office for several years. Zerona was originally developed to assist in the performance of traditional lipo. \"I was involved with the company that developed Zerona...,\" says Dr. Barnes. \"I've been using this since 2001, thousands of cases. We know it causes fat to leak from the cells without injuring the cells. It makes the lipo easier. We knew that; then the company went to the next step and was able to take the same technology, and found that if you put it on the skin to affect the fat longer than 6 or 8 minutes - like we use with lipo when the fluid is in there - we can get the same result, but we need longer time on skin to affect the fat below it.\"\n",
      "Dr. Barnes says, \"The Zerona is a new technology available now for what I call body slimming.... Keep in mind, they're having fat leave fat cells through a transitory pore that develops under the influence of this particular non-invasive laser light that doesn't destroy tissue.\"\n",
      "Though the laser may be aimed at one part of the body, patients and physicians are often seeing inch reduction in areas other than those treated. \"We can lose 4 or 5 inches, an aggregate, off of hips, thighs and waist.... I shine the lights there, 20 minutes on the front, 20 minutes on the back, you feel nothing. So your clothes fit better, it's a motivator for a new weight loss program... You also slim off the upper arms, knees and neck, even though those are outside the area we shine the lights.\"\n",
      "Zeltiq, on the other hand is not a laser at all. It uses cold therapy to provide spot fat reduction. Dr. Lori Brightman of the Laser & Skin Surgery Center of New York is working with this device that uses a technology known as cryolipolysis. Dr. Brightman informs us, \"[Zeltiq is] actually extracting heat from the fat cells, and your body's natural reaction to the inflammation that's created is to slowly dissolve those fat cells over time.\" She advises, \"This is a good treatment for love handles, back fat, little lower poochy belly.\"\n",
      "The treatment is not painful. As Dr. Brightman describes, \"It's not very uncomfortable. It just feels like a cold application on your skin's surface, and a suction.\" The treatment itself is pretty straight forward. \"You take the applicator and you put it on the patient's area that they're targeting. It creates a suction which pulls the skin in to the suction applicator, and then they'll start to feel the cooling of their skin.\" She says the applicator stays on for one hour per applicator, but different areas may require different time commitments.\n",
      "Dr. Brightman's patients are seeing about 20% reduction in the fat layer per treatment, but cautions it does take about 4 months to see results. \"You're seeing this localized spot of fat reduced over time because it's your body's natural response to the inflammation that's created, so you'll see a slow resolution of that pocket of fat.\"\n"
     ]
    }
   ],
   "source": [
    "print(urls[0])\n",
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f16a1e-0208-45fc-9616-aef2820e9335",
   "metadata": {},
   "source": [
    "#### Let's compute the number of distinct domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cea2731c-d63b-40f6-91f9-db9956c4178e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_domain(url):\n",
    "    url = url.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    domain = url.split(\"/\")[0]\n",
    "    return domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "708e6fed-7dcd-4443-b08c-8d2411827f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of domains: 841520\n",
      "Number of domains with at least 4 pages: 49076\n"
     ]
    }
   ],
   "source": [
    "domains = {}\n",
    "\n",
    "for url in urls:\n",
    "    domain = get_domain(url)\n",
    "    if domain not in domains:\n",
    "        domains[domain] = 0\n",
    "    domains[domain] += 1\n",
    "\n",
    "print(f\"Number of domains: {len(domains)}\")\n",
    "print(f\"Number of domains with at least 4 pages: {sum([1 for (domain, x) in domains.items() if x >= 4])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8151f7-d640-4769-90e0-258480457a54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2905979-8cbf-456e-bf6f-a29ce9d69aaa",
   "metadata": {},
   "source": [
    "#### Let's build a simple inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "738c1b17-7310-4bda-a11b-2229f028ea81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/rossano/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rossano/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/rossano/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Download necessary data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e9942bdc-502a-49be-9cdf-941891cf5af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(doc):   \n",
    "    # Step 1: Convert document to lowercase\n",
    "    doc = doc.lower()\n",
    "\n",
    "    # Step 2: # Step 1: Normalize accents e.g., café vs cafe\n",
    "    doc = unidecode(doc)\n",
    "    \n",
    "    # Step 3: Normalize multiple spaces to a single space\n",
    "    doc = \" \".join(doc.split())\n",
    "    \n",
    "    # Step 4: Tokenize the document\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # Step 5: Remove punctuation\n",
    "    tokens_no_punct = [word for word in tokens if word not in string.punctuation]\n",
    "    \n",
    "    # Step 6: Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_no_stopwords = [word for word in tokens_no_punct if word not in stop_words]\n",
    "    \n",
    "    # Step 7: Lemmatization (or stemming)\n",
    "    #lemmatizer = WordNetLemmatizer()\n",
    "    #tokens_lemmatized = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]\n",
    "    \n",
    "    # Step 8: Remove numbers\n",
    "    tokens_no_numbers = [word for word in tokens_no_stopwords if not word.isdigit()]\n",
    "\n",
    "    return tokens_no_numbers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa0942bd-da90-4509-ac37-151f2d6fe56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'undertakes',\n",
       " 'many',\n",
       " 'forms',\n",
       " 'fundraising',\n",
       " 'raise',\n",
       " 'funds',\n",
       " 'programmes',\n",
       " 'kenya',\n",
       " 'include',\n",
       " 'organising',\n",
       " 'events',\n",
       " 'selling',\n",
       " 'goods',\n",
       " 'donations',\n",
       " 'standing',\n",
       " 'orders',\n",
       " 'grant',\n",
       " 'applications',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'committed',\n",
       " 'achieving',\n",
       " 'standards',\n",
       " 'outlined',\n",
       " 'statement',\n",
       " 'guiding',\n",
       " 'principles',\n",
       " 'fundraising',\n",
       " 'supplied',\n",
       " 'charities',\n",
       " 'institute',\n",
       " 'ireland',\n",
       " 'organisation',\n",
       " 'representing',\n",
       " 'interests',\n",
       " 'irish',\n",
       " 'charities',\n",
       " 'good',\n",
       " 'practice',\n",
       " 'standards',\n",
       " 'fundraising',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " \"'s\",\n",
       " 'accounts',\n",
       " 'comply',\n",
       " 'statement',\n",
       " 'recommended',\n",
       " 'practice',\n",
       " 'sorp',\n",
       " 'standard',\n",
       " 'general',\n",
       " 'dochas/irish',\n",
       " 'aid',\n",
       " 'guidelines',\n",
       " 'financial',\n",
       " 'reporting',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'publishes',\n",
       " 'annual',\n",
       " 'accounts',\n",
       " 'line',\n",
       " 'every',\n",
       " 'year',\n",
       " 'available',\n",
       " 'website',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'registered',\n",
       " 'charities',\n",
       " 'regulatory',\n",
       " 'authority',\n",
       " 'complied',\n",
       " 'reporting',\n",
       " 'obligations',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'governed',\n",
       " 'board',\n",
       " 'directors',\n",
       " 'ireland',\n",
       " 'board',\n",
       " 'management',\n",
       " 'kenya',\n",
       " 'role',\n",
       " 'boards',\n",
       " 'govern',\n",
       " 'organisation',\n",
       " 'accordance',\n",
       " 'mission',\n",
       " 'vision',\n",
       " 'legal',\n",
       " 'obligations',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'implemented',\n",
       " 'dochas',\n",
       " 'code',\n",
       " 'ngo',\n",
       " 'corporate',\n",
       " 'governance',\n",
       " 'reviews',\n",
       " 'compliance',\n",
       " 'annual',\n",
       " 'basis',\n",
       " 'comply',\n",
       " 'governance',\n",
       " 'code',\n",
       " 'community',\n",
       " 'voluntary',\n",
       " 'charitable',\n",
       " 'organisations',\n",
       " 'ireland',\n",
       " 'confirm',\n",
       " 'review',\n",
       " 'organisation',\n",
       " \"'s\",\n",
       " 'compliance',\n",
       " 'principles',\n",
       " 'code',\n",
       " 'conducted',\n",
       " 'june',\n",
       " '2017.',\n",
       " 'review',\n",
       " 'based',\n",
       " 'assessment',\n",
       " 'organisational',\n",
       " 'practice',\n",
       " 'recommended',\n",
       " 'actions',\n",
       " 'principle',\n",
       " 'review',\n",
       " 'sets',\n",
       " 'actions',\n",
       " 'completion',\n",
       " 'dates',\n",
       " 'issues',\n",
       " 'assessment',\n",
       " 'identifies',\n",
       " 'need',\n",
       " 'addressed',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'listed',\n",
       " 'compliance',\n",
       " 'list',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'conflict',\n",
       " 'interest',\n",
       " 'policy',\n",
       " 'conflict',\n",
       " 'loyalty',\n",
       " 'policy',\n",
       " 'place',\n",
       " 'potential',\n",
       " 'conflicts',\n",
       " 'interest',\n",
       " 'loyalty',\n",
       " 'dealt',\n",
       " 'executive',\n",
       " 'committee',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'compliant',\n",
       " 'terms',\n",
       " 'ngo',\n",
       " 'coordination',\n",
       " 'act',\n",
       " 'attendant',\n",
       " 'regulations',\n",
       " 'kenya',\n",
       " 'confirmed',\n",
       " 'ngo',\n",
       " 'coordination',\n",
       " 'board',\n",
       " 'january',\n",
       " 'view',\n",
       " 'letter',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'board',\n",
       " 'members',\n",
       " 'voluntary',\n",
       " 'receive',\n",
       " 'payments',\n",
       " 'attend',\n",
       " 'board',\n",
       " 'meetings',\n",
       " 'expenses',\n",
       " 'incurred',\n",
       " 'board',\n",
       " 'members',\n",
       " 'fulfilling',\n",
       " 'duties',\n",
       " 'board',\n",
       " 'members',\n",
       " 'paid',\n",
       " 'according',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " \"'s\",\n",
       " 'expense',\n",
       " 'policy',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'pay',\n",
       " 'pension',\n",
       " 'allowances',\n",
       " 'employees',\n",
       " 'prsa',\n",
       " 'scheme',\n",
       " 'place',\n",
       " 'employee',\n",
       " 'wish',\n",
       " 'avail',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'welcomes',\n",
       " 'feedback',\n",
       " 'governance',\n",
       " 'transparency',\n",
       " 'please',\n",
       " 'contact',\n",
       " 'office',\n",
       " 'using',\n",
       " 'contact',\n",
       " 'details',\n",
       " 'click',\n",
       " 'see',\n",
       " 'codes',\n",
       " 'good',\n",
       " 'practice',\n",
       " 'brighter',\n",
       " 'communities',\n",
       " 'worldwide',\n",
       " 'adhere',\n",
       " 'various',\n",
       " 'organisations',\n",
       " 'affiliated']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens(documents[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a24224d-cdaa-4a34-a2fa-b29d3061dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_map = {}\n",
    "frequencies = [] # for ith term, frequencies[i] stores its frequency\n",
    "df = [] # for ith term, df[i] stores the number of documents containing this term\n",
    "posting_lists = [] # list of lists. In real life, it is a big list with offsets. (Lists in Python are vectors!)\n",
    "tfs = [] # list of lists. For each document, the frequency of the term in the document\n",
    "\n",
    "for doc_id, document in enumerate(documents[:10]):\n",
    "    tokens = get_tokens(document)\n",
    "    \n",
    "    for token in tokens: \n",
    "        if token not in vocabulary_map: # new term\n",
    "            vocabulary_map[token] = len(vocabulary_map)\n",
    "            posting_lists.append([])\n",
    "            tfs.append([])\n",
    "            frequencies.append(0)\n",
    "            df.append(0)\n",
    "            \n",
    "        token_id = vocabulary_map[token]\n",
    "        frequencies[token_id] += 1\n",
    "        \n",
    "        if len(posting_lists[token_id]) == 0 or posting_lists[token_id][-1] != doc_id: # avoid duplicated doc_ids within the same list\n",
    "            posting_lists[token_id].append( doc_id )\n",
    "            tfs[token_id].append( 0 )\n",
    "            df[token_id] += 1\n",
    "            \n",
    "        tfs[token_id][-1] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "24f5b3fe-04f5-4d18-8f1e-2f467de565cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 12, 1, 1, 1, 3, 2, 1, 1, 1, 15, 1, 2, 1]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequencies[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8358d60-649d-457b-b5d6-f89154800ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('liposuction', 0),\n",
       " ('remained', 1),\n",
       " ('one', 2),\n",
       " ('popular', 3),\n",
       " ('cosmetic', 4),\n",
       " ('surgeries', 5),\n",
       " ('years', 6),\n",
       " ('people', 7),\n",
       " ('turn', 8),\n",
       " ('doctors', 9),\n",
       " ('remove', 10),\n",
       " ('fat', 11),\n",
       " ('diet', 12),\n",
       " ('exercise', 13),\n",
       " ('ca', 14)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocabulary_map.items(), key = lambda x: x[1])[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b9ee7b8-555d-48ab-8806-2ae548116ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 3), (1, 2), (3, 1), (4, 1), (5, 2), (6, 2), (9, 1)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term = \"one\"\n",
    "\n",
    "term_id = vocabulary_map[term]\n",
    "\n",
    "list( zip(posting_lists[term_id], tfs[term_id]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06b18e16-8d70-4f9e-855f-2751e3e45931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 3\n"
     ]
    }
   ],
   "source": [
    "doc_id = posting_lists[term_id][0]\n",
    "freq = tfs[term_id][0]\n",
    "\n",
    "print( freq, documents[doc_id].count(term) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa96b89-dc44-4c13-877d-21bfc6f65d8d",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "## Data Compression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4900f3-9031-47d8-8b31-1d0fdbd79f4f",
   "metadata": {},
   "source": [
    "#### Reorder documents sorting by URLs\n",
    "\n",
    "**Goal**: Assign close *docIds* to documents from the same domain. **Why?**\n",
    "\n",
    "Let's reverse the domain so that:\n",
    "\n",
    "https://microsoft.github.io --> io.github.microsoft "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b197c1af-7744-4f33-b9d1-2e1a754759de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://microsoft.github.io --> io.github.microsoft\n",
    "def get_domain_rev(domain):\n",
    "    domain = domain.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    return \".\".join(domain.split(\".\")[::-1])\n",
    "\n",
    "def get_url_rev(url):\n",
    "    url = url.replace(\"https://\", \"\").replace(\"http://\", \"\")\n",
    "    split = url.split(\"/\")\n",
    "    return get_domain_rev(split[0]) + \"/\" + \"/\".join(split[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "750d42dd-2ba6-46a0-8a0f-c6d05deb5a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "io.github.microsoft\n"
     ]
    }
   ],
   "source": [
    "print(get_domain_rev(\"https://microsoft.github.io\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d71470a-d0eb-4abb-8bf0-9921a5e6d657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "io.github.microsoft/presidio/getting_started/\n"
     ]
    }
   ],
   "source": [
    "print(get_url_rev(\"https://microsoft.github.io/presidio/getting_started/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e717ca6-134a-4806-8c2e-7016e07c9528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://americanhealthandbeauty.com/articles/2704/non-surgical-fat-reduction--zerona-vs-zeltiq\n",
      "com.americanhealthandbeauty/articles/2704/non-surgical-fat-reduction--zerona-vs-zeltiq\n"
     ]
    }
   ],
   "source": [
    "print(urls[0])\n",
    "print(get_url_rev(urls[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cb47e8e-7172-4769-965d-f623cf14eb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "com.americanhealthandbeauty/articles/2704/non-surgical-fat-reduction--zerona-vs-zeltiq\n"
     ]
    }
   ],
   "source": [
    "urls_rev = [get_url_rev(url) for url in urls]\n",
    "\n",
    "print(urls_rev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80a76991-1ae8-4ae1-83d8-f51abe99601f",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation = list(range(len(urls)))\n",
    "\n",
    "permutation.sort(key = lambda i: get_url_rev(urls[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6d7fd0fc-4665-4afb-8bc8-eeab3c6a6920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.accc.gov.au./media-release/accc-holiday-operations\n"
     ]
    }
   ],
   "source": [
    "print(urls[permutation[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a3c0e7-641e-4855-867a-caee9c3f6d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
